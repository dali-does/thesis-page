# Site
repository: dali-does/thesis-page

version: 2
favicon: fa-fire

# Personal info
name: Adam Dahlgren Lindstrom
title: PhD student, Umeå University
email: dali@cs.umu.se
website: https://www.umu.se/en/staff/adam-dahlgren-lindstrom/
# website_title: Web (Website title override)

# Dark Mode (true/false/never)
darkmode: false

# Social links
#twitter_username: jekyllrb
github_username:  dali-does
orcid_username: 0000-0002-1112-2981
googlescholar_username: LctkPQgAAAAJ


# About Section
about_title: Description of Thesis - Defense on September 15th 2023
about_profile_image: https://www.umu.se/contentassets/82f461f9a9e44e6ba31f462e79da1849/Profilbild.jpg?v=1283924955
about_content: |
  Hi, my name is Adam Dahlgren Lindström, this is a website to keep relevant parties informed on the progress of my thesis writing. I am interested in <mark>language grounding</mark> in <mark>multimodal machine learning</mark>, <mark>neuro-symbolic</mark> methods, and <mark>compositional generalisation</mark>.

  This thesis is on the effects of including vision in learning language, and investigates how vision-centric concepts are represented, how reasoning tasks on multimodal data can help us understand language models, and how we can leverage vision for compositional generalisation.  I am writing a monographic thesis, with parts of chapters based on the papers listed below.

  You can find the <mark>most up to date version of the thesis here:</mark> <a href="thesis.pdf">thesis draft (March 14)</a>

content:
  - title: Research Questions
    layout: list
    content:
      - layout: left
        border: weak
        title: Research Question 1
        description: |
          What are the contributions of introducing vision to language modeling?
      - layout: left
        border: weak
        title: Research Question 2
        description: |
          What are advantages and disadvantages of neuro-symbolic methods in multimodal language processing?
      - layout: left
        border: weak
        title: Research Question 3
        description: |
          Does utilizing hierarchical structures improve compositional generalisation in language grounding?
      - layout: left
        border: weak
        title: Research Question 4
        description: |
          What are the challenges and opportunities for language-centric learning on multimodal data, and what future research directions are there?

  - title: Published work
    layout: list
    content:
      - layout: left
        border: weak
        title: COLING 2020
        sub_title: 'Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic case'
        link: https://aclanthology.org/2020.coling-main.64/
        link_text: Read Paper Here
        caption: Probing
        description: |
          Semantic embeddings have advanced the state of the art for countless natural language processing tasks, and various extensions to multimodal domains, such as visual-semantic embeddings, have been proposed. While the power of visual-semantic embeddings comes from the distillation and enrichment of information through machine learning, their inner workings are poorly understood and there is a shortage of analysis tools. To address this problem, we generalize the notion of probing tasks to the visual-semantic case. To this end, we (i) discuss the formalization of probing tasks for embeddings of image-caption pairs, (ii) define three concrete probing tasks within our general framework, (iii) train classifiers to probe for those properties, and (iv) compare various state-of-the-art embeddings under the lens of the proposed probing tasks. Our experiments reveal an up to 12% increase in accuracy on visual-semantic embeddings compared to the corresponding unimodal embeddings, which suggest that the text and image dimensions represented in the former do complement each other.
      - layout: left
        border: weak
        title: EMNLP 2021
        sub_title: Bridging Perception, Memory, and Inference through Semantic Relations
        link: https://aclanthology.org/2021.emnlp-main.719/
        link_text: Read Paper Here
        caption: Probing
        description: |
          There is a growing consensus that surface form alone does not enable models to learn meaning and gain language understanding. This warrants an interest in hybrid systems that combine the strengths of neural and symbolic methods. We favour triadic systems consisting of neural networks, knowledge bases, and inference engines. The network provides perception, that is, the interface between the system and its environment. The knowledge base provides explicit memory and thus immediate access to established facts. Finally, inference capabilities are provided by the inference engine which reflects on the perception, supported by memory, to reason and discover new facts. In this work, we probe six popular language models for semantic relations and outline a future line of research to study how the constituent subsystems can be jointly realised and integrated.
      - layout: left
        border: weak
        title: NeSy@IJCLR 2022
        sub_title: 'CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning'
        link: https://arxiv.org/abs/2208.05358
        link_text: Read Paper Here
        caption: Compositional Generalisation, Neuro-symbolic
        description: |
          We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.
      - layout: left
        border: weak
        title: AAAI Fall Symposium 2022
        sub_title: Thinking Fast and Slow in Human-Centered AI
        link: https://hal.inria.fr/hal-03991946/document
        link_text: Read Paper Here
        caption: Neuro-symbolic
        description: |
          Thinking Fast and Slow (Kahneman 2011) provides a simple mental model of how human intelligence builds on components with complementing responsibilities and capabilities. In computer science in general, and artificial intelligence research in particular, these ideas are used to inspire new methods and architectures. We argue that many of those methods use the concept of Thinking Fast and Slow as a token reference, while not living up to the definitions of dual-process systems from psychology. For instance, 'fast' is seen as synonymous with neural, and the autonomy of 'fast' seen in, e.g., fixed social interactions of social agents is lost. We further highlight that these ideas are misused in saying that humans are flawed and AI systems can fix that. Given that human bias is highly context-dependent, such simplistic applications of dual-process theory to AI are likely to fail. Thus, the narrative that AI systems will provide users with rationality is flawed. In a work in progress, we survey and categorise (mis)use of prospect theory and other dual-process theories. Building AI systems on the ideas of Tversky and Kahneman is a step in the right direction for a more human-centered artificial intelligence. With this work we want to emphasise the many things to consider in building systems that are Thinking Fast and Slow, and that the community is only scratching that surface.

  - title: Unpublished work
    layout: list
    content:
      - layout: left
        border: weak
        title: TBA
        sub_title: Learning hierarchical concepts for compositional generalisation
        caption: Compositional Generalisation, Probing
        description: |
          We build a benchmark for compositional generalisation on top of a synthetic concept hierarchy. We use the benchmark to investigate effects of curriculum learning, scaling laws, and compositional generalisation characteristics. We use probing to examine the internal structures of models.
      - layout: left
        border: weak
        title: TBA
        sub_title: 'Confounding DeepProbLog with colored digits - neuro-symbolic methods and compositional generalisation'
        caption: Compositional Generalisation
        description: |
          One key argument for neuro-symbolic methods is that they should be able to generalise compositionally. In these experiments, we show how a common method, DeepProbLog, is easily confused by coloring MNIST digits during training.

  - title: Papers not included in thesis
    layout: list
    content:
      - layout: left
        border: weak
        title: NSNLI@IJCAI 2021
        sub_title: 'Perception, Memory, and Inference: The Trinity of Machine Learning'
        link: https://nsnli.github.io/assets/Paper_ID5.pdf
        link_text: Read Paper Here
        caption: Neuro-symbolic
        description: |
          As an answer to recent contributions about the conjectured impossibility of learning meaning from surface form alone,and the dangers of large language models, we argue in this paper that an explicit distinction should be made between (i) perception, (ii) memory, and (iii) inference.  We envision a triad of interacting subsystems with corresponding responsibilities.  Perception provides the interface between  the  system  and  its  environment,  and  is  typically  realised as a language model.  Explicit Memory is a structure of concepts and relations between the concepts, in other words, a knowledge base of facts.   Inference,  finally,  corresponds to mathematical or rule-based reasoning and provides,  for ex-ample, classical logic and arithmetic. We note here that such axiomatic systems cannot be deduced from data,  only conjectured and tried against data.  Our position is that natural language systems should thus combine continuously updated language and other perception models (e.g., computer vision)with one or more symbolic knowledge bases that relieve the models from learning concepts and relations, and finally one or more inference engines to provide formal reasoning.
      - layout: left
        border: weak
        title: LearnAut@ICALP 2022
        sub_title: An Algebraic Approach to Learning and Grounding
        caption: Grounding
        link: https://arxiv.org/pdf/2204.02813.pdf
        link_text: Read Paper Here
        description: |
          'We consider the problem of learning the semantics of composite algebraic expressions from examples. The outcome is a versatile framework for studying learning tasks that can be put into the following abstract form: The input is a partial algebra and a finite set of examples, each consisting of an algebraic term and a set of objects. The objective is to simultaneously fill in the missing algebraic operations in $\alg$ and ground the variables of every in term in objects, so that the combined value of the terms is optimised. We demonstrate the applicability of this framework through case studies in grammatical inference, picture-language learning, and the grounding of logic scene descriptions.'
      - layout: left
        border: weak
        title: FAccT 2023
        sub_title: 'ACROCPoLis: A Descriptive Framework for Making Sense of Fairness'
        caption: Fairness
        description: |
          Accepted, final submission not yet available.



# Footer
footer_show_references: false

# Build settings
remote_theme: sproogen/resume-theme

# sass:
#   style: compressed
